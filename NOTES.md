# TODO
- sdfs server
    - replicate and re-balance blocks
    - append file
- file server
    - append block
- sdfs client
    - append file

- maplejuice
    - reduce job, reduce task
    - use sdfs client at leader to download and send blocks to workers
    - worker use sdfs client to append data to file (prefix\_key)

- sdfs server:
    - client heartbeat service

- client - caching on retry


Sat Nov 18 08:35:38 AM CST 2023

- maplejuice balance nummaples across worker nodes
- testing
- add reduce job


Sun Nov 26 12:37:50 AM CST 2023

- send tasks in chunks for better performance
- check issue with invalid file size in reduce job?
- download executables in worker, not executor
- test worker failure and fix issues
- try different mappers and reducers, with large dataset
- test on VM


Fri Dec  1 08:49:34 PM CST 2023

Report

MP4 Report
Aarya Bhatia aaryab2@illinois.edu, Ricky Hsu rickyhh2@illinois.edu

Architecture

Each node runs a process with an SDFS, MapleJuice RPC server, and Failure Detector. The FD notifies the SDFS and MJ servers through a callback function when a node joins or leaves the system.

MP3 updates

API to write to a file in two modes: Truncate and Append
API to read n bytes from a file at a given offset

MP4 Leader:
Has a queue of jobs (MapJob, ReduceJob) which are run sequentially
Each job generates some tasks (MapTask, ReduceTask)
A map job fetches and reads input file shards to split files into batches of <=100 lines each, each batch is a MapTask. The workers only receive the task info, they read the file on their own using the SDFS client library
A reduce job lists all the input files matching prefixes. Also, each key is split over multiple files (for parallelism), so we group files by key, and each group becomes a ReduceTask
Each task is assigned to a particular worker through a partition function (hash, range)
A job is run over three phases: StartJob, Task phase, FinishJob
The Start phase informs the workers to prepare for the job, download the executable (maple_exe, juice_exe), and launch a pool of N executors (num_maples, num_juices) running as goroutines.
The Task phase is where the leader sends out tasks and waits for an Ack from the workers for each task
In the Finish phase, the output key-value pairs aggregated at the worker are flushed out to SDFS. Each worker's output file is different so that they can run in parallel. Later the output files are concatenated to get the complete output. The leader expects an Ack from each worker.
At the end of the Finish phase or beginning of the start phase, the executors are killed or restarted.

Worker:
A worker node runs a pool of executors as goroutines and communicates to them through a channel. The pool is initialized during the StartJob phase and destroyed after the FinishJob phase.
An executor receives a task from the worker runs the corresponding executable program in a subprocess and pipes the stdin and stdout of the program.
The input to a map program is some lines of text. The input to a reduce program is a list of lines containing some values, for a given key. The executable is also passed the filename and key as arguments.
The output from a map and reduce program is some lines containing "key: value" pairs.
The output from the executable is parsed and merged into a hashmap of key:values[]. Eventually, this is written out to an SDFS file, with the name "outputFile:workerID".

Failure handling:
On worker failure during the Task phase, the leader quickly reschedules the tasks of the worker to the remaining workers using the same hash function. If there are no more workers, the job will fail. A failed job is retried upto 3 times.
If a worker fails during the start phase, it is no longer considered for the job.
If a worker fails during the finish phase, we retry the job, after waiting for the remaining workers to finish what they are doing.
The output files generated by a failed worker are also removed.


MapReduce

We write a map and reduce executable file (language agnostic) that can follow the input and output format described before. The executable must be uploaded to SDFS before the job.
The client can pass an arbitrary number of CLI args to an executable. These args apply to all the executables during the job. The system also supplies some args such as the filename of the input file and the key in the case of a reducer.
For filter queries, the map emits those lines that match the given regex pattern. The key is the filename and the value is the matching line. The reducer is identity.
For join queries, the map emits tuples like "column:filename,other  fields…" for each row where the column matches a pattern. The reducer can join the rows for each column and emit the final row as "column:file1,fields of file1…,file2,fields of file2…".

Experiments

1. (Filter 1, Filter 2) Pick TWO examples of Filter queries (one simple, one complex regex)
2. (Join 1, Join 2) TWO examples of Join queries (one simple, one complex).
3. For #1 above, vary the cluster size (number of VMs) up to 10. For #2 above, vary data size (use all 10 VMs).

Ensure you have at least 5 data points on the x axis.

